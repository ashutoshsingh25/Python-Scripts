{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import nltk\n",
    "from bs4 import BeautifulSoup\n",
    "from nltk.tokenize import WordPunctTokenizer\n",
    "from nltk.corpus import stopwords \n",
    "from nltk.tokenize import word_tokenize \n",
    " \n",
    "#Downloading the common english language stopwords from the nltk module in Python\n",
    "#nltk.download('stopwords')\n",
    "stop_words = stopwords.words('english')\n",
    "\n",
    "\n",
    "#Adding the extra stopwords identified as per business use-case into the above set \n",
    "stop_words.extend((\"for medicine\",\"medicine\",\"for medicines\",\"medicines\",\"mg\",\"mgs\",\"ml\",\"mls\",\"kg\",\"kgs\",\"degree\",\"degrees\",\"g\",\"gms\",\"gm\",\"mm\",\"gram\",\"grams\",\"ft\",\"cm\",\"cms\",\"m\",\"cu\",\"we\",\"are\",\"dealing\",\"quality\",\"manufacturers\",\"manufacturer\",\"exporters\",\"supplier\",\"dealer\",\"good\",\"topmost\",\"business\",\"trusted\",\"finest\",\"offer\",\"offering\",\"involved\",\"provide\",\"reputed\",\"company\",\"organization\",\"trader\",\"trading\",\"li\",\"pvt.\",\"ltd\",\"pvt\",\"ltd.\"))\n",
    "\n",
    "#Buylead specific stopwords\n",
    "stop_words.extend([\"i\",\"want\",\"to\",\"buy\",\"setup\",\"am\",\"looking\",\"service provider\",\"need\",\"will\",\"samples\",\"before\",\"purchasing\",\"see\",\"product\",\"starting\",\"business venture\",\"kindly\",\"share\",\"details\",\"via\",\"whatsapp\",\"whatsaap\",\"sms\",\"email\",\"know\",\"price\",\"requirement\",\"send\",\"interested\",\"my\",\"good quality\",\"would\",\"would like\",\"feet\",\"bore\",\"size\",\"mr\",\"rs\",\"per\",\"inch\",\"indian rupee\",\"total order value\",\"rupee\",\"l\",\"xl\",\"xxl\",\"also\",\"usage\",\"basis\",\"use\",\"reselling\",\"buying\",\"nos\",\"discussed\",\"personal\",\"personally\",\"installation\",\"required\",\"quote\",\"asap\",\"indiamart\",\"what\",\"where\",\"why\",\"how\",\"then\",\"decide\",\"kindly\",\"piece\",\"stock\",\"length\",\"usd\",\"diameter\",\"lot\",\"kilogram\",\"upto\",\"km\",\"peices\",\"indian\",\"purchase\",\"meter\",\"medium\",\"pair\",\"like\",\"full\",\"lakh\",\"rupees\",\"delivery\",\"rpm\",\"litre\",\"ton\",\"easy\",\"yes\",\"thickness\",\"composite\",\"liter\",\"including\",\"pack\",\"venture\",\"marketing\",\"features\",\"condition\",\"hour\",\"high\",\"age group\",\"packaging\",\"dimension\",\"star rating\",\"making\",\"searching\",\"products\",\"services\",\"things\",\"resell\",\"suppliers\"])\n",
    "\n",
    "stop_words1 = set(stopwords.words('english')) \n",
    "\n",
    "token1 = WordPunctTokenizer()\n",
    "\n",
    "#Defining regular expression for special characters, numerics and hyperlinks \n",
    "ditits_and_num1 = r'@[A-Za-z0-9_]+'\n",
    "hyperlink_2 = r'https?://[^ ]+'\n",
    "numeric_3 = r'[0-9]+'\n",
    "combined_pattern1 = r'|'.join((ditits_and_num1, hyperlink_2,numeric_3))\n",
    "www_pattern = r'www.[^ ]+'\n",
    "pat_3 = r'[^A-Za-z0-9]+'\n",
    "\n",
    "#Transforming negative words into simple words\n",
    "negative_word_list = {\"isn't\":\"is not\", \"aren't\":\"are not\", \"wasn't\":\"was not\", \"weren't\":\"were not\",\n",
    "                \"haven't\":\"have not\",\"hasn't\":\"has not\",\"hadn't\":\"had not\",\"won't\":\"will not\",\n",
    "                \"wouldn't\":\"would not\", \"don't\":\"do not\", \"doesn't\":\"does not\",\"didn't\":\"did not\",\n",
    "                \"can't\":\"can not\",\"couldn't\":\"could not\",\"shouldn't\":\"should not\",\"mightn't\":\"might not\",\n",
    "                \"mustn't\":\"must not\"}\n",
    "\n",
    "\n",
    "neg_pattern = re.compile(r'\\b(' + '|'.join(negative_word_list.keys()) + r')\\b')\n",
    "\n",
    "\n",
    "def cleaned_and_processed(text):\n",
    "    b_soup = BeautifulSoup(text, 'lxml')\n",
    "    b_souped = b_soup.get_text()\n",
    "    try:\n",
    "        b_removed = b_souped.decode(\"utf-8-sig\").replace(u\"\\ufffd\", \"?\")\n",
    "    except:\n",
    "        b_removed = b_souped\n",
    "    \n",
    "    stripped_w = re.sub(combined_pattern1, '', b_removed)\n",
    "    stripped_w = re.sub(www_pattern, '', stripped_w)\n",
    "    stripped_w = re.sub(pat_3, '#', stripped_w)\n",
    "    stripped_w = re.sub(':',' ',stripped_w)\n",
    "    stripped_w = re.sub('::',' ',stripped_w)\n",
    "    lower_case = stripped_w.lower()\n",
    "    neg_handled = neg_pattern.sub(lambda x: negative_word_list[x.group()], lower_case)\n",
    "    \n",
    "    # To handle letters_only cases process two lines above, it has created unnecessay white spaces,\n",
    "    # tokenizing and joining together to remove unneccessary white spaces\n",
    "    words = [x for x  in token1.tokenize(neg_handled) if len(x) > 1]\n",
    "    return (\" \".join(words)).strip()\n",
    "\n",
    "\n",
    "def clean(text):\n",
    "    cleanr = re.compile('<.*?>')\n",
    "    text = re.sub(cleanr, '', text)\n",
    "    text = re.sub(r'[^\\w\\s]',\" \",text)\n",
    "    text = re.sub(\"@\",\" \",text)\n",
    "    text = re.sub(\"[0-9] \\\\w+ *\",\" \",text)\n",
    "    text = re.sub(\"[0-9] \\\\w+ *\",\" \",text)\n",
    "    text = re.sub(\" +\",\" \",text)\n",
    "    text = text.lower()\n",
    "    return text\n",
    "\n",
    "def stop_words(text):\n",
    "    text = ''.join([i for i in text if not i.isdigit()])\n",
    "    text = ' '.join([word for word in text.split() if word not in stop_words])\n",
    "    return(text)\n",
    "\n",
    "def rem_dup(l):\n",
    "    ulist = []\n",
    "    [ulist.append(x) for x in l if x not in ulist]\n",
    "    return ulist\n",
    "#def testFuncNew():\n",
    " #   text = 'hello bye the the hi'\n",
    "  #  text = ' '.join([word for word in text.split() if word not in cachedStopWords])\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!/usr/bin/env python\n",
    "# coding: utf-8\n",
    "\n",
    "# In[ ]:\n",
    "\n",
    "\n",
    "import flask\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import fastText\n",
    "import re\n",
    "from preprocess import * # Here we are importing all the funcation from user defined library, created as per the use case\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "import time\n",
    "\n",
    "stop_words = stopwords.words('english')\n",
    "\n",
    "\n",
    "#Adding the extra stopwords identified as per business use-case into the above set \n",
    "stop_words.extend((\"for medicine\",\"medicine\",\"for medicines\",\"medicines\",\"mg\",\"mgs\",\"ml\",\"mls\",\"kg\",\"kgs\",\"degree\",\"degrees\",\"g\",\"gms\",\"gm\",\"mm\",\"gram\",\"grams\",\"ft\",\"cm\",\"cms\",\"m\",\"cu\",\"we\",\"are\",\"dealing\",\"quality\",\"manufacturers\",\"manufacturer\",\"exporters\",\"supplier\",\"dealer\",\"good\",\"topmost\",\"business\",\"trusted\",\"finest\",\"offer\",\"offering\",\"involved\",\"provide\",\"reputed\",\"company\",\"organization\",\"trader\",\"trading\",\"li\",\"pvt.\",\"ltd\",\"pvt\",\"ltd.\"))\n",
    "\n",
    "#Buylead specific stopwords\n",
    "stop_words.extend([\"i\",\"want\",\"to\",\"buy\",\"setup\",\"am\",\"looking\",\"service provider\",\"need\",\"will\",\"samples\",\"before\",\"purchasing\",\"see\",\"product\",\"starting\",\"business venture\",\"kindly\",\"share\",\"details\",\"via\",\"whatsapp\",\"whatsaap\",\"sms\",\"email\",\"know\",\"price\",\"requirement\",\"send\",\"interested\",\"my\",\"good quality\",\"would\",\"would like\",\"feet\",\"bore\",\"size\",\"mr\",\"rs\",\"per\",\"inch\",\"indian rupee\",\"total order value\",\"rupee\",\"l\",\"xl\",\"xxl\",\"also\",\"usage\",\"basis\",\"use\",\"reselling\",\"buying\",\"nos\",\"discussed\",\"personal\",\"personally\",\"installation\",\"required\",\"quote\",\"asap\",\"indiamart\",\"what\",\"where\",\"why\",\"how\",\"then\",\"decide\",\"kindly\",\"piece\",\"stock\",\"length\",\"usd\",\"diameter\",\"lot\",\"kilogram\",\"upto\",\"km\",\"peices\",\"indian\",\"purchase\",\"meter\",\"medium\",\"pair\",\"like\",\"full\",\"lakh\",\"rupees\",\"delivery\",\"rpm\",\"litre\",\"ton\",\"easy\",\"yes\",\"thickness\",\"composite\",\"liter\",\"including\",\"pack\",\"venture\",\"marketing\",\"features\",\"condition\",\"hour\",\"high\",\"age group\",\"packaging\",\"dimension\",\"star rating\",\"making\",\"searching\",\"products\",\"services\",\"things\",\"resell\",\"suppliers\"])\n",
    "\n",
    "\n",
    "# instantiate flask \n",
    "app = flask.Flask(__name__)\n",
    "\n",
    "# load the model, and pass in the custom metric function\n",
    "global graph\n",
    "graph = tf.get_default_graph()\n",
    "\n",
    "\n",
    "# define a predict function as an endpoint \n",
    "@app.route(\"/predict\", methods=[\"GET\",\"POST\"])\n",
    "def predict():\n",
    "    data = {\"success\": False}\n",
    "\n",
    "    params = flask.request.json\n",
    "    if (params == None):\n",
    "        params = flask.request.args\n",
    "\n",
    "\n",
    "        # if parameters are found, return a prediction\n",
    "    if (params != None):\n",
    "        x = params.get(\"msg\")\n",
    "        Start = round(time.time() * 1000)\n",
    "        x = cleaned_and_processed(x)\n",
    "        x = clean(x)\n",
    "        #cleanr = re.compile('<.*?>')\n",
    "        #x = re.sub(cleanr, '', x)\n",
    "        #x = re.sub(r'[^\\w\\s]',\" \",x)\n",
    "        x = ' '.join(rem_dup(x.split()))\n",
    "        x = ''.join([i for i in x if not i.isdigit()])\n",
    "        x = ' '.join([word for word in x.split() if word not in stop_words])\n",
    "        l=[x]\n",
    "\n",
    "\n",
    "    data[\"prediction\"]=re.sub(\"__label__\",\"\",model.predict(l,k=1)[0][0][0])\n",
    "    data[\"confidence\"] = model.predict(l,k=1)[1][0][0]\n",
    "    data[\"success\"] = True\n",
    "    data[\"input_string\"] = x\n",
    "    End = round(time.time() * 1000)\n",
    "    \n",
    "    data[\"time_taken\"] = str(End - Start) + str(\" ms\")\n",
    "    #data[\"prob\"]=str(model.predict(word_seq_test)[0][0])\n",
    "\n",
    "\n",
    "    # return a response in json format \n",
    "    return flask.jsonify(data)    \n",
    "\n",
    "# start the flask app, allow remote connections \n",
    "if __name__ == '__main__':\n",
    "    model_name='Banned_ML_Model'\n",
    "    model = fastText.load_model(model_name+\".bin\")\n",
    "    app.run(host='0.0.0.0')\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
